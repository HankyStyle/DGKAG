{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformer 來做 Triplet 的分類任務 (Positive / Negative)\n",
    "任務說明:\n",
    "給定一段句子與 triplet，判斷 triplet 是否為 positive 或是 negative\n",
    "1. Positive (0) 定義 : 與 distractor 相關的 triplet<br>\n",
    "2. Negative (1) 定義 : 與 distractor 不相關的 triplet<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 29 02:26:12 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX                On | 00000000:01:00.0 Off |                  N/A |\n",
      "| 40%   38C    P8               20W / 280W|      1MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN RTX                On | 00000000:02:00.0 Off |                  N/A |\n",
      "| 40%   33C    P8               34W / 280W|      1MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight and Bias (Assisting Metrics, Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"test Sentence Transformer on MCQ reranker\"\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = project_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 02:26:19.516966: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-29 02:26:19.680014: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-29 02:26:20.157900: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-29 02:26:20.158019: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-29 02:26:20.158027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(item):\n",
    "    path = '../../../../data/mcq/reranker.{}.json'.format(item)\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('train')\n",
    "test = read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2321, 259)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentence', 'distractors', 'answer', 'triplets'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**blank** is used to describe a chemical released by an animal that affects the behavior or physiology of animals of the same species\n",
      "['enzyme', 'isolate', 'amino']\n",
      "pheromone\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['isa', 'protease', 'enzyme', 2.0],\n",
       " ['relatedto', 'used', 'use', 2.0],\n",
       " ['isa', 'enzyme', 'catalyst', 2.0],\n",
       " ['relatedto', 'catalyst', 'enzyme', 1.0],\n",
       " ['relatedto', 'carbohydrate', 'animal', 1.0],\n",
       " ['relatedto', 'pheromone', 'chemical', 1.0],\n",
       " ['relatedto', 'protease', 'enzyme', 1.0],\n",
       " ['relatedto', 'animal', 'animals', 1.0],\n",
       " ['relatedto', 'affects', 'affect', 1.0],\n",
       " ['relatedto', 'pheromone', 'species', 1.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Triplet with Sentence Transformer\n",
    "print(test[0]['sentence'])\n",
    "print(test[0]['distractors'])\n",
    "print(test[0]['answer'])\n",
    "test[0]['triplets'][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relation_Dict ={\n",
    " 'antonym': 'is the antonym of',\n",
    " 'atlocation' : 'is at location of',\n",
    " 'capableof': 'is capable of',\n",
    " 'causes' : 'causes',\n",
    " 'createdby': 'is created by',\n",
    " 'desires': 'desires',\n",
    " 'hasproperty': 'has property',\n",
    " 'hassubevent': 'has subevent',\n",
    " 'isa':'is a kind of',\n",
    " 'madeof':'is made of',\n",
    " 'notcapableof':'has not capable of',\n",
    " 'notdesires': \"does not desires\",\n",
    " 'partof':'is part of',\n",
    " 'receivesaction':'is',\n",
    " 'relatedto':'is related to',\n",
    " 'usedfor':'is used for'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2088, 233)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, valid = train_test_split(train, random_state=777, train_size=0.9)\n",
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data):\n",
    "    \n",
    "    sentences = []\n",
    "    triplets = []\n",
    "    labels = []\n",
    "    answers = []\n",
    "    for d in data:\n",
    "\n",
    "        sentence = d['sentence']\n",
    "        distractors = d['distractors']\n",
    "        answer = d['answer']\n",
    "        triplet = d['triplets']\n",
    "\n",
    "\n",
    "        distractors = [dis.strip() for dis in distractors]\n",
    "        sentence = sentence.replace('**blank**', answer)\n",
    "\n",
    "        \n",
    "        for each_triplet in triplet:\n",
    "            rel, source, target, weight = each_triplet\n",
    "\n",
    "            sentences.append(sentence)\n",
    "            triplets.append('{} {} {}'.format(source,Relation_Dict[rel],target))\n",
    "\n",
    "            if source == answer or target == answer or source in distractors or target in distractors:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "            \n",
    "              \n",
    "        \n",
    "    return sentences, triplets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent, train_triplet, train_label = processData(train)\n",
    "valid_sent, valid_triplet, valid_label = processData(valid)\n",
    "test_sent, test_triplet, test_label = processData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37740, 4114, 4390)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sent), len(valid_sent), len(test_sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "統計訓練與測試資料分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 資料分布 : Positive 有 8616 筆, Negative 有 29124 筆，大約有 22.83% 為 Positive。\n",
      "Valid 資料分布 : Positive 有 923 筆, Negative 有 3191 筆，大約有 22.44% 為 Positive。\n",
      "Test 資料分布 : Positive 有 1346 筆, Negative 有 3044 筆，大約有 30.66% 為 Positive。\n"
     ]
    }
   ],
   "source": [
    "print('Train 資料分布 : Positive 有 {} 筆, Negative 有 {} 筆，大約有 {:.2f}% 為 Positive。'.format(train_label.count(0),train_label.count(1),train_label.count(0)/len(train_sent)*100))\n",
    "print('Valid 資料分布 : Positive 有 {} 筆, Negative 有 {} 筆，大約有 {:.2f}% 為 Positive。'.format(valid_label.count(0),valid_label.count(1),valid_label.count(0)/len(valid_sent)*100))\n",
    "print('Test 資料分布 : Positive 有 {} 筆, Negative 有 {} 筆，大約有 {:.2f}% 為 Positive。'.format(test_label.count(0),test_label.count(1),test_label.count(0)/len(test_sent)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_sent, train_triplet, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_sent, valid_triplet, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_sent, test_triplet, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2003, 7146, 23899, 1014, 2000, 19448, 2103, 4080, 2300, 6153, 3437, 1016, 2, 2, 8764, 2007, 3145, 2004, 7146, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<s> in continuous reinforcement, the reinforcer follows every correct response. </s> </s> sustained is related to continuous </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'][0])\n",
    "print(tokenizer.decode(train_encodings['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(encodings, label):\n",
    "    encodings.update({'labels': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_labels(train_encodings, train_label)\n",
    "add_labels(valid_encodings, valid_label)\n",
    "add_labels(test_encodings, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['labels'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義 Dataset，並轉換成 tensor 格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings):\n",
    "    self.encodings = encodings\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = Dataset(train_encodings)\n",
    "valid_dataset = Dataset(valid_encodings)\n",
    "test_dataset = Dataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  2003,  7146, 23899,  1014,  2000, 19448,  2103,  4080,  2300,\n",
       "          6153,  3437,  1016,     2,     2,  8764,  2007,  3145,  2004,  7146,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor(0)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentence-Transformers model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/all-mpnet-base-v2 were not used when initializing MPNetForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MPNetConfig, MPNetForSequenceClassification\n",
    "config  = MPNetConfig.from_pretrained('sentence-transformers/all-mpnet-base-v2', num_labels = 2) # num_labels 設定類別數\n",
    "model = MPNetForSequenceClassification.from_pretrained('sentence-transformers/all-mpnet-base-v2', config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看模型架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNetForSequenceClassification(\n",
      "  (mpnet): MPNetModel(\n",
      "    (embeddings): MPNetEmbeddings(\n",
      "      (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): MPNetEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (relative_attention_bias): Embedding(32, 12)\n",
      "    )\n",
      "  )\n",
      "  (classifier): MPNetClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "   \n",
    "    results = metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {'accuracy': results['accuracy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    weight_decay=0.01,\n",
    "    eval_accumulation_steps = 1,\n",
    "    report_to=\"wandb\" if os.getenv(\"WANDB_PROJECT\") else \"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 69438\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10850\n",
      "  Number of trainable parameters = 109488002\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhankystyle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/user_data/DG-via-Text2Text-with-Triplet-Augmentation/reranker/wandb/run-20230527_134223-e7d9b3s0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hankystyle/test%20on%20MCQ%20reranker/runs/e7d9b3s0' target=\"_blank\">cerulean-dream-8</a></strong> to <a href='https://wandb.ai/hankystyle/test%20on%20MCQ%20reranker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hankystyle/test%20on%20MCQ%20reranker' target=\"_blank\">https://wandb.ai/hankystyle/test%20on%20MCQ%20reranker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hankystyle/test%20on%20MCQ%20reranker/runs/e7d9b3s0' target=\"_blank\">https://wandb.ai/hankystyle/test%20on%20MCQ%20reranker/runs/e7d9b3s0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='10850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   33/10850 00:07 < 45:42, 3.94 it/s, Epoch 0.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-1085\n",
      "Configuration saved in ./results/checkpoint-1085/config.json\n",
      "Model weights saved in ./results/checkpoint-1085/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1085/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1085/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-2170\n",
      "Configuration saved in ./results/checkpoint-2170/config.json\n",
      "Model weights saved in ./results/checkpoint-2170/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2170/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2170/special_tokens_map.json\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-3255\n",
      "Configuration saved in ./results/checkpoint-3255/config.json\n",
      "Model weights saved in ./results/checkpoint-3255/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3255/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3255/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2170] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-4340\n",
      "Configuration saved in ./results/checkpoint-4340/config.json\n",
      "Model weights saved in ./results/checkpoint-4340/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4340/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4340/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3255] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-5425\n",
      "Configuration saved in ./results/checkpoint-5425/config.json\n",
      "Model weights saved in ./results/checkpoint-5425/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5425/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5425/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-4340] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-6510\n",
      "Configuration saved in ./results/checkpoint-6510/config.json\n",
      "Model weights saved in ./results/checkpoint-6510/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6510/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6510/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-5425] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-7595\n",
      "Configuration saved in ./results/checkpoint-7595/config.json\n",
      "Model weights saved in ./results/checkpoint-7595/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-7595/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7595/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-6510] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-8680\n",
      "Configuration saved in ./results/checkpoint-8680/config.json\n",
      "Model weights saved in ./results/checkpoint-8680/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-8680/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8680/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-7595] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-9765\n",
      "Configuration saved in ./results/checkpoint-9765/config.json\n",
      "Model weights saved in ./results/checkpoint-9765/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9765/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9765/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-8680] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-10850\n",
      "Configuration saved in ./results/checkpoint-10850/config.json\n",
      "Model weights saved in ./results/checkpoint-10850/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-10850/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-10850/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-9765] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-1085 (score: 0.8836416109143382).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10850, training_loss=0.0886934984004992, metrics={'train_runtime': 3663.7041, 'train_samples_per_second': 189.529, 'train_steps_per_second': 2.961, 'total_flos': 4.53179295641628e+16, 'train_loss': 0.0886934984004992, 'epoch': 10.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='121' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 16:57:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.30712223052978516,\n",
       " 'eval_accuracy': 0.8836416109143382,\n",
       " 'eval_runtime': 22.2692,\n",
       " 'eval_samples_per_second': 342.311,\n",
       " 'eval_steps_per_second': 5.389,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 8339\n",
      "  Batch size = 64\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.3614237308502197,\n",
       " 'test_accuracy': 0.8732461925890395,\n",
       " 'test_runtime': 5.2905,\n",
       " 'test_samples_per_second': 1576.222,\n",
       " 'test_steps_per_second': 24.761}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(test_dataset)\n",
    "print('test: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/sentence-transformer-mcq\n",
      "Configuration saved in ./saved_models/sentence-transformer-mcq/config.json\n",
      "Model weights saved in ./saved_models/sentence-transformer-mcq/pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/sentence-transformer-mcq/tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/sentence-transformer-mcq/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('../../../../saved_models/Reranker/sentence-transformer-for-mcq')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence =  pheromone is used to describe a chemical released by an animal that affects the behavior or physiology of animals of the same species\n",
      "triplet =  pheromone is related to species\n"
     ]
    }
   ],
   "source": [
    "sentence = test_sent[0]\n",
    "triplet = test_triplet[0]\n",
    "print('sentence = ',sentence)\n",
    "print('triplet = ',triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MPNetConfig, MPNetForSequenceClassification\n",
    "config  = MPNetConfig.from_pretrained('../../../../saved_models/Reranker/sentence-transformer-for-mcq', num_labels = 2)\n",
    "model = MPNetForSequenceClassification.from_pretrained('../../../../saved_models/Reranker/sentence-transformer-for-mcq', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encodings = tokenizer([sentence], [triplet], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = Dataset(input_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  6891, 10628,  8206,  2067,  2007,  2113,  2004,  6239,  1041,\n",
       "          5076,  2211,  2015,  2023,  4115,  2012, 13535,  2000,  5252,  2034,\n",
       "         16131,  2001,  4180,  2001,  2000,  2172,  2431,     2,     2,  6891,\n",
       "         10628,  8206,  2067,  2007,  3145,  2004,  2431,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation 分布情況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 7623\n",
      "  Batch size = 64\n",
      "You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.30712223052978516,\n",
       " 'test_accuracy': 0.8836416109143382,\n",
       " 'test_runtime': 24.2069,\n",
       " 'test_samples_per_second': 314.91,\n",
       " 'test_steps_per_second': 4.957}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(valid_dataset)\n",
    "print('valid: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3894/2714362684.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  {'accuracy': 0.8836416109143382}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "print('accuracy : ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.51      0.61      1373\n",
      "           1       0.90      0.97      0.93      6250\n",
      "\n",
      "    accuracy                           0.88      7623\n",
      "   macro avg       0.83      0.74      0.77      7623\n",
      "weighted avg       0.88      0.88      0.87      7623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for line in classification_report(labels, predictions).split('\\n'):\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 703  670]\n",
      " [ 217 6033]]\n"
     ]
    }
   ],
   "source": [
    "# **產生 confusion matrix heatmap **\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "cf_matrix = confusion_matrix(labels, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5630356514978342\n"
     ]
    }
   ],
   "source": [
    "matthews = matthews_corrcoef(labels, predictions)\n",
    "print(matthews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Predicted', ylabel='True Label'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAHhCAYAAADAst+7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAn0UlEQVR4nO3dd5gW1dn48e8tJaICYkOaLWIsiS12NBYMIsYaG74qKnkxRn2NaRr11dhNMXajWLGhoBgwapTYS1AsWDEv/EyUagMBSwIs5/fHM+Cqu8siO8+sO99PrufamTPnmTkT99qb+5SZSCkhSZKqY5miGyBJUpkYeCVJqiIDryRJVWTglSSpigy8kiRVkYFXkqQqMvBKkkopIlaMiDsj4o2IGB8R20bEShExOiImZD87ZXUjIi6NiIkR8XJEbF7rPAOy+hMiYsBir9tc1/GuvfImzbNh0hKqSQuKboK01N6e8Urkde5577/Z5H/v26yyzmLbGxFDgCdSStdGRFtgOeAUYEZK6YKIOBnolFI6KSL6AccD/YCtgUtSSltHxErAc8AWQAKeB76bUppZ33XNeCVJpRMRHYHvAdcBpJTmppQ+BPYGhmTVhgD7ZNt7AzelijHAihHRBdgNGJ1SmpEF29FA34au3bqJ70WSpCWzoKbJTxkRg4BBtYoGp5QG19pfG3gPuCEiNqGSqZ4AdE4pTcvqTAc6Z9vdgEm1vj85K6uvvF4GXklSsXIYjsmC7OAGqrQGNgeOTyk9ExGXACd/4RwpIpq8G9yuZklSGU0GJqeUnsn276QSiN/JupDJfr6bHZ8C9Kj1/e5ZWX3l9TLwSpKKtWBB038WI6U0HZgUEd/KinoDrwOjgIUzkwcAI7PtUcDh2ezmbYBZWZf0A0CfiOiUzYDuk5XVy65mSVJZHQ/cms1ofhM4kkpCOiwiBgJvAQdmde+jMqN5IvBJVpeU0oyIOBsYm9U7K6U0o6GLupxIypnLidQS5LmcaO7U15r8733brhvl1t6lZcYrSSpWI7qGWxLHeCVJqiIzXklSsUo2HGPGK0lSFZnxSpKKlcOTq5ozM15JkqrIjFeSVKySjfEaeCVJxXI5kSRJyosZrySpUKlkXc1mvJIkVZEZrySpWCUb4zXwSpKKZVezJEnKixmvJKlYPrlKkiTlxYxXklSsko3xGnglScUq2axmu5olSaoiM15JUrFK1tVsxitJUhWZ8UqSilWyMV4DrySpUCm5jleSJOXEjFeSVCwnV0mSpLyY8UqSilWyyVVmvJIkVZEZrySpWCUb4zXwSpKK5WsBJUlSXsx4JUnFKllXsxmvJElVZMYrSSpWyZYTGXglScWyq1mSJOXFjFeSVKySdTWb8UqSVEVmvJKkYpUs4zXwSpIKlZJPrpIkSTkx45UkFatkXc1mvJIkVZEZrySpWD5AQ5Ik5cWMV5JUrJKN8Rp4JUnFsqtZkiTlxYxXklSsknU1m/FKklRFZrySpGKVbIzXwCtJKpZdzZIkKS9mvJKkYpnxSpKkvJjxSpKK5eQqSZKqyK5mSZKUFzNeSVKxStbVbMYrSVIVmfFKkorlGK8kScqLGa8kqVglG+M18EqSimVXsyRJyosZrySpWGa8kiS1fBHxr4h4JSLGRcRzWdlKETE6IiZkPztl5RERl0bExIh4OSI2r3WeAVn9CRExYHHXNfBKkoqVUtN/Gm/nlNKmKaUtsv2TgYdSSj2Bh7J9gN2BntlnEPAnqARq4Axga2Ar4IyFwbo+Bl5JUrEWLGj6z1e3NzAk2x4C7FOr/KZUMQZYMSK6ALsBo1NKM1JKM4HRQN+GLmDglSS1OBExKCKeq/UZVEe1BDwYEc/XOt45pTQt254OdM62uwGTan13clZWX3m9nFwlSSpWDpOrUkqDgcGLqbZ9SmlKRKwGjI6IN75wjhQRS9Rv3RhmvJKkUkopTcl+vgvcTWWM9p2sC5ns57tZ9SlAj1pf756V1VdeLwOvJKlYaUHTfxYjIpaPiPYLt4E+wKvAKGDhzOQBwMhsexRweDa7eRtgVtYl/QDQJyI6ZZOq+mRl9bKrWZJUrGLW8XYG7o4IqMTC21JKf42IscCwiBgIvAUcmNW/D+gHTAQ+AY4ESCnNiIizgbFZvbNSSjMaurCBV5JUOimlN4FN6ij/AOhdR3kCjq3nXNcD1zf22gZeSVKxlmzd7deeY7ySJFWRGa8kqVg+q1mSJOXFjFeSVKySZbwGXklSsRqx7rYlsatZkqQqMuOVJBUqLXA5kSRJyokZrySpWE6ukiSpipxcJUmS8mLGK0kqlpOrJElSXsx4JUnFcnKVJElVVLLAa1ezJElVZMYrSSpWcnKVJEnKiRmvJKlYjvFKkqS8GHi/5tZZd03uffSORZ+X//UURx79X0t1zv0O3pOHnx3Fw8+OYr+D9wRg2XbLct3Qy/jbmD/zwFMj+NXpJzRF81VSHTq056obL+ThMaN4aMxINt9yk88dP/r4I7j/seHc/9hwRj81gn++N46OK3ZYqmu2bduGK677PY8/dy8jR99K9x5dAdhhp2259+E7ePDJEdz78B1st8NWS3UdfQULUtN/mrFIzXRQe+2VN2meDWvGlllmGca8Opp9+xzKlMnTFlt/6Mhr+cVxpzNl0tRFZR1X7MCoh4ayV+/+pJS45+Hb2XOXg5k7dx6bfvc7jHlyLG3atObWu6/hiouu5bGHnsrzllqEmpI9h7Yx/njFOTw75gVuv3kEbdq0pl27dsyePafOurvutiMDjzmM/vv8qFHn7t6jKxdecQ4H7XXU58oPO+ogNthoPU75+dnsuV9f+u7Rm2MH/pKNvrM+77/3Ae9Mf4/1NliXW4ZfxVbf3nWp77GleXvGK5HXuT/5/VFN/vd+uV9en1t7l5YZbwvS63tb89a/JjFl8jTWWKs7Nw67klEPDWXYX25gnZ5rNeoc39tlO558dAyzPpzN7FlzePLRMezYuxf//vTfjHlyLADz5s3n1ZfH06Vr5xzvRi1V+/YrsNV23+X2m0cAld+n+oIuwF4/7MeoEfcv2t/3gB8wavRt3P/YcM7/4+kss0zj/oz16bczd94+CoD7Ro6m1/e2BuC1V97gnenvAfB/4yeybLtladu2zVe6N6kxDLwtyA/268s9I/4KwHkXnc5vTr6AvXr357wz/sjZvz+1UedYvctqTJs6fdH+9KnvsHqX1T5Xp32H9vTebUeeevyZpmu8SqPHmt2Y8f5MLrz8HO57dBi/veQ3tFuuXZ11l223LDv17sV9o0YDsO56a7Pnvrux3+6Hs/uOB1BTU8O+B+zRqOuu3mU1pk6p/G7X1NQwZ/ZHdFppxc/V6bfX93n1pfHMnTvvq9+gllzJuppzm9UcEesDewPdsqIpwKiU0vi8rllmbdq0Zte+O/L7sy9hueXb8d0tN+GK63+/6Hjbb7QFYP9D9ubIQYcAsObaa3DDHZczb+48Jr09lR8ffuJir9OqVSsuveYCbhx8G5PempLPzahFa926Fd/eZANOP/l8xj3/Cr85/yR+8tOBXHje5V+q+/2+O/LcMy8y68PZAPT63jZ8Z5MNueehoQAsu+w3+OD9GQAMvulieqzZjbZt29C1Wxfuf2w4ANdffSvDb/vzYtu13vrf5NdnnMihPxzURHcq1S2XwBsRJwH9gduBZ7Pi7sDQiLg9pXRBPd8bBAwCWHm5brRfduU8mtci7bTr9rz28hu8/94MVmi/PLNnzWGPnQ76Ur07bxvJnbeNBOoe450+7V226bXlov3Vu3ZmzFNjF+2fd9Hp/OvNt7nh6ltzvBu1ZNOmvsO0qe8w7vlXgEq37zE/HVhn3T333Z2Rd33WzRwR3Hn7KH579iVfqjvo8J8C9Y/xTp/2Ll27rc70qe/QqlUr2ndYgZkzPgQqv+eDb7qYE39yCm/9a3IT3KWWRHI5UZMYCGyZUrogpXRL9rkA2Co7VqeU0uCU0hYppS0Muktmz/12XzQO9tGcj5n09hT67fX9Rcc32Gi9Rp3n8YefZoedt6VDx/Z06NieHXbelscffhqAn59yLO07rMBZp/yu6W9ApfHeux8wbcp01ll3LQB67bg1E/7x/75Ur337Fdim1xY8eP8ji8qeenwM/fb6PiuvshJQmQzYrXuXRl139P2Psv/BewHQb+/v8/QTlZygQ4f23Hj7FVxw1sU898y4pbgzfWUl62rOK/AuALrWUd4lO6Ym1G65dmy/0zY8cM9Di8p+evQpHHjovtz32DAefHoEu+6+c6PONevD2Vz2h8GM/NttjPzbbVz6h6uZ9eFsVu+6Gsf9fBA9v7UOf3nkdu599A4OOnTfvG5JLdzpJ53PpVdfwANP3MWG316fK/54LYcecQCHHnHAojq7/aA3jz/yNJ9+8umisgn/eJM/nHcZt9x1NQ88cRe3jbiG1VZftVHXvOOWEXRaaUUef+5e/vuYw7ngzIsBGPDf/Vlr7R6c8MsfL1rCtDCwS3nIZTlRRPQFLgcmAJOy4jWAdYHjUkp/Xdw5XE6klsLlRGoJ8lxO9PE5hzb53/vlT7ul2S4nymWMN6X014hYj0rXcu3JVWNTSjV5XFOSpK+D3GY1p5QWAGPyOr8kqYVo5mOyTc2XJEiSiuWsZkmSlBczXklSsUrW1WzGK0lSFZnxSpKKVbIld2a8kiRVkRmvJKlYJRvjNfBKkgrlSxIkSVJuzHglScUqWVezGa8kSVVkxitJKlbJMl4DrySpWK7jlSRJeTHjlSQVq2RdzWa8kiRVkRmvJKlQqWQZr4FXklSskgVeu5olSaoiM15JUrF8VrMkScqLGa8kqViO8UqSpLyY8UqSilWyjNfAK0kqVErlCrx2NUuSVEVmvJKkYpWsq9mMV5KkKjLjlSQVq2QZr4FXklSosr0kwa5mSZKqyIxXklQsM15JkpQXM15JUrHK9XIiM15JUrHSgtTkn8aKiFYR8WJE/CXbXzsinomIiRFxR0S0zcq/ke1PzI6vVescv87K/xERuy3umgZeSVKZnQCMr7X/W+CilNK6wExgYFY+EJiZlV+U1SMiNgQOBjYC+gJXRkSrhi5o4JUkFWtBavpPI0REd2AP4NpsP4BdgDuzKkOAfbLtvbN9suO9s/p7A7enlP6TUvonMBHYqqHrGnglSS1ORAyKiOdqfQbVUe1i4Fd8Nsq8MvBhSml+tj8Z6JZtdwMmAWTHZ2X1F5XX8Z06OblKklSsHCZXpZQGA4PrOx4RPwDeTSk9HxE7NX0L6mfglSSVUS9gr4joBywLdAAuAVaMiNZZVtsdmJLVnwL0ACZHRGugI/BBrfKFan+nTnY1S5IKVcSs5pTSr1NK3VNKa1GZHPVwSum/gEeA/bNqA4CR2faobJ/s+MOp8iLhUcDB2azntYGewLMNXduMV5JUrOa1jvck4PaIOAd4EbguK78OuDkiJgIzqARrUkqvRcQw4HVgPnBsSqmmoQtEJWA3P2uvvEnzbJi0hGpS8/qrIn0Vb894JfI698wf7tTkf+873fVobu1dWma8kqRC+XYiSZKUGzNeSVKxSjYaY+CVJBWqbNMg7GqWJKmKzHglScUy45UkSXkx45UkFapsY7wGXklSsUoWeO1qliSpisx4JUmFKltXsxmvJElVZMYrSSpU2TJeA68kqVBlC7x2NUuSVEVmvJKkYqVm++rcXJjxSpJURWa8kqRCOcYrSZJyY8YrSSpUWlCuMV4DrySpUHY1S5Kk3JjxSpIKlVxOJEmS8mLGK0kqVNnGeA28kqRClW1Ws13NkiRVkRmvJKlQKRXdguoy45UkqYrMeCVJhSrbGK+BV5JUqLIFXruaJUmqIjNeSVKhnFwlSZJyY8YrSSqUY7ySJCk39Wa8EbF5Q19MKb3Q9M2RJJVN2d5O1FBX84UNHEvALk3cFklSCfmShExKaedqNkSSpDJY7OSqiFgO+BmwRkppUET0BL6VUvpL7q2TJLV4C0rW1dyYyVU3AHOB7bL9KcA5ubVIkqQWrDHLib6ZUjooIvoDpJQ+iYhy/fNEkpQbJ1d92dyIaEdlQhUR8U3gP7m2SpJUGmVbx9uYwHsG8FegR0TcCvQCjsizUZIktVSLDbwppdER8QKwDRDACSml93NvmSSpFMr2rObGPjJyR2B7Kt3NbYC7c2uRJEktWGOWE10JrAsMzYqOjohdU0rH5toySVIpOMb7ZbsAG6SUFk6uGgK8lmurJEml4TreL5sIrFFrv0dWJkmSllBDL0m4h8qYbntgfEQ8m+1vDTxbneZJklo61/F+5g9Va4UkSSXR0EsSHqtmQyRJ5VS25USLHeONiG0iYmxEfBQRcyOiJiJmV6NxkiS1NI2Z1Xw5cDAwHNgCOBxYL89GSZLKw1nNdUgpTQRapZRqUko3AH3zbZYkqSxSiib/NGeNyXg/iYi2wLiI+B0wjUYGbEmS9HmNCaCHZfWOAz6mso53vzwbJUkqj5Sa/tOcNeYlCW9lm/8GzgSIiDuAg3JslyRJLVJjX5LwRds2aSskSaVVtslVXzXw5m7SHN88qJbh06lPFN0EqVlr7pOhmlpDj4zcvL5DVF4NKEmSllBDGe+FDRx7o6kbIkkqJ7uaMymlnavZEEmSyqDZjvFKksqhma/+aXIGXklSocrW1ewTqCRJqqLGvJ0oIuLQiDg9218jIrbKv2mSpDIo4lnNEbFsRDwbES9FxGsRsfABUWtHxDMRMTEi7sgemUxEfCPbn5gdX6vWuX6dlf8jInZb3LUbk/FeSeWBGf2z/TnAFY34niRJzdV/gF1SSpsAmwJ9I2Ib4LfARSmldYGZwMCs/kBgZlZ+UVaPiNiQyhv8NqLyAqErI6JVQxduTODdOqV0LJVHRpJSmgm0XaLbkySpHgty+CxOqvgo222TfRKwC3BnVj4E2Cfb3jvbJzveOyIiK789pfSflNI/gYlAg73CjQm887LonQAiYtVG3pckSYWIiEER8Vytz6A66rSKiHHAu8Bo4P8BH6aU5mdVJgPdsu1uwCSA7PgsYOXa5XV8p06NmdV8KXA3sFpEnAvsD5zWiO9JkrRYiaaf1ZxSGgwMXkydGmDTiFiRSpxbv8kbUofGvJ3o1oh4HuhN5XGR+6SUxufeMklSKSwoeCFvSunDiHiEynymFSOidZbVdgemZNWmUHkt7uSIaA10BD6oVb5Q7e/UqTGzmtcAPgHuAUYBH2dlkiR9LUXEqlmmS0S0A74PjAceodKzCzAAGJltj8r2yY4/nFJKWfnB2azntYGewLMNXbsxXc33UhnfDWBZYG3gH1RmcEmStFQW5NDV3AhdgCHZHKZlgGEppb9ExOvA7RFxDvAicF1W/zrg5oiYCMygMpOZlNJrETEMeB2YDxybdWHXqzFdzd+pvZ+9tegnS3J3kiQ1Jymll4HN6ih/kzpmJaeU/g0cUM+5zgXObey1l/iRkSmlFyJi6yX9niRJdcljclVzttjAGxE/q7W7DLA5MDW3FkmSSqVs61Mbk/G2r7U9n8qY7135NEeSpJatwcCbDTq3Tyn9okrtkSSVTNm6mutdTpStY6oBelWxPZIktWgNZbzPUhnPHRcRo4DhwMcLD6aURuTcNklSCTjG+2XLUnk6xy58tp43AQZeSdJSM/B+ZrVsRvOrfBZwFyr4AV+SJH09NRR4WwErQJ2j3gZeSVKTKNvkqoYC77SU0llVa4kkSSXQUOAt1z9BJEmFWFCyaNPQ24l6V60VkiSVRL0Zb0ppRjUbIkkqp4LeTlSYJX5JgiRJTalss3Ub6mqWJElNzIxXklSosj1Aw4xXkqQqMuOVJBVqQTi5SpKkqnFylSRJyo0ZrySpUE6ukiRJuTHjlSQVqmzPajbwSpIKVbZHRtrVLElSFZnxSpIK5XIiSZKUGzNeSVKhyja5yoxXkqQqMuOVJBWqbA/QMPBKkgrl5CpJkpQbM15JUqGcXCVJknJjxitJKpSTqyRJqqKyBV67miVJqiIzXklSoZKTqyRJUl7MeCVJhSrbGK+BV5JUqLIFXruaJUmqIjNeSVKhfFazJEnKjRmvJKlQPqtZkiTlxoxXklSoss1qNvBKkgpVtsBrV7MkSVVkxitJKpTLiSRJUm7MeCVJhSrbciIDrySpUE6ukiRJuTHjlSQVyslVkiQpN2a8kqRCLShZzmvglSQVyslVkiQpN2a8kqRClauj2YxXkqSqMuOVJBXKMV5JkpQbM15JUqF8VrMkSVVUtnW8djVLkkonInpExCMR8XpEvBYRJ2TlK0XE6IiYkP3slJVHRFwaERMj4uWI2LzWuQZk9SdExIDFXdvAK0kqVMrh0wjzgZ+nlDYEtgGOjYgNgZOBh1JKPYGHsn2A3YGe2WcQ8CeoBGrgDGBrYCvgjIXBuj4GXklS6aSUpqWUXsi25wDjgW7A3sCQrNoQYJ9se2/gplQxBlgxIroAuwGjU0ozUkozgdFA34au7RivJKlQeSwniohBVDLThQanlAbXU3ctYDPgGaBzSmladmg60Dnb7gZMqvW1yVlZfeX1MvBKkgqVx+SqLMjWGWhri4gVgLuAn6aUZkd8NsU6pZQioskbZ1ezJKmUIqINlaB7a0ppRFb8TtaFTPbz3ax8CtCj1te7Z2X1ldfLwCtJKlQRk6uiktpeB4xPKf2x1qFRwMKZyQOAkbXKD89mN28DzMq6pB8A+kREp2xSVZ+srF52NUuSyqgXcBjwSkSMy8pOAS4AhkXEQOAt4MDs2H1AP2Ai8AlwJEBKaUZEnA2MzeqdlVKa0dCFDbySpEIV8azmlNKTQH3PzOpdR/0EHFvPua4Hrm/stQ28kqRC+eQqSZKUGzNeSVKhypXvmvFKklRVZrySpEIVMbmqSAZeSVKhUsk6m+1qliSpisx4JUmFKltXsxmvJElVZMYrSSqUD9CQJEm5MeOVJBWqXPmugVeSVDC7miVJUm7MeCVJhXI5kZqt7t278rcHh/PyS4/w0riHOf64gV+q07//vrzw/GhefOFvPPHYSDbeeMOlvm7btm257dY/8cbrT/L0k/ew5prdAdi19w48M+Z+Xnzhbzwz5n523qnXUl9L5TV7zkeceOo57Nn/v9nzkEGMe3X8Up1v5H2j6XfQQPodNJCR941eVH70z05jvwE/Ye//Opozf3cZNTU1S9t0aYkYeL9G5s+fzy9/dSYbb7Izvbbfk2OOOYINNuj5uTr/+uckdum9P5ttvivnnncxV13520aff801u/PQ6OFfKj/qyP7MnDmL9TfcnosvvYbzzzsVgPc/mME++x7BZpvvylEDf8qNN1yydDeoUrvg4qvotfUW3DP0GkYMuYJ11uzRqO8dcdyvmDLtnc+VzZo9hz/dcBtDr7mYoddczJ9uuI1Zs+cAcOHZv2bEkCv58y1XMfPDWTzwyBNNfi9aMimH/zVnBt6vkenT3+XFca8C8NFHH/PGGxPo1nX1z9X5+5jn+PDDWQCMeeYFunXrsujYIYfsx9+f+gvPjX2QK6/4Lcss07j//Hvt2Yebb64E5Lvuupdddt4egHHjXmNa9gfvtdf+Qbt2y9K2bdulu0mV0pyPPub5l17lh3vuBkCbNm3o0H4F3p48laN/dhoHHnU8hx/zC958a1KjzvfUM8+z7Zab0bFDezp2aM+2W27GU888D8AKyy8PwPyaGubNn0cQ+dyUGm1BDp/mzMD7NbXmmt3ZdJNv88yzL9Zb56gjD+avDzwCwPrrr8uBB+zFDjvuwxZb9qGmpoZDDtmvUdfq2m11Jk2eCkBNTQ2zZs1m5ZU7fa7OfvvtwYsvvsrcuXO/4h2pzKZMnU6nFTty2rl/ZP8jjuX08y/mk0//zZm/u5RTTjyGYddfxi+O+xHn/OGKRp3vnffeZ/XVVl2033nVVXjnvfcX7Q868VR2/EF/ll9uOfpk/5CUqqXqk6si4siU0g31HBsEDAKIVh1ZZpnlq9q2r4vll1+OYXdcw89+cQZz5nxUZ52ddtyOI4/sz4477QvALjtvz+abfYcxf78PgHbtluW97A/RncOvZa211qBt2zas0aMbz419EIDLLruWITcNW2x7NtxwPc4/9xR23+OQprg9ldD8mhrG/99ETjnxGDbeaH3Ov/gqLhs8hHGvjOdnp523qN7cefMAuPveB7ll2EgA3p4ylWN+8b+0ad2Gbl07c+n5py/2eoMvOpf//GcuJ535O555/iW222rzfG5MjdLcu4abWhGzms8E6gy8KaXBwGCA1m27leu/RCO1bt2a4Xdcw9Chd/PnP99fZ53vfGcDrr7q9/xgr8OYMWMmABHBzbcM59TTLvhS/f0P+BFQyaKvv/Yien//gM8dnzplOj26d2XKlGm0atWKjh078MEHlfN269aFO4dfx5FHncCbb77VlLeqEll9tVXovOoqbLzR+gD02Wl7Lr/2Ztq3X567hnw5y913jz7su0cfoDLGe+6pP6dbl86LjndedRXGvvjyov133nufLTfb+HPn+MY32rLzDtvwyBNjDLyqqly6miPi5Xo+rwCdF3sC1euawRcy/o2JXHzJ4DqP9+jRleF3XMMRR57AhAlvLip/+JEn2W/fH7DqqisD0KnTiqyxRrdGXfOevzzIYYdVgvEPf7gHjzz6FAAdO3Zg1MibOOXU83j6788tzW2p5FZZeSVWX21V/vnWZADGPD+OjdbvSbcuq/PAw5XJTykl3qj1O92QXlt/l6effYFZs+cwa/Ycnn72BXpt/V0++eRT3nt/BgDz59fw+NNjWTubpa/ilG2MN6+MtzOwGzDzC+UBPJ3TNVu8XtttyWGH7s/Lr7y+qDv4f//3Anr0qATQwdfczGmnnsjKK3fisssq3XPz589nm237MX78BE7/ze+4/76hLLNMMG/efP7nf07l7benLPa6199wO0NuvJQ3Xn+SmTM/5JBDfwLAsT85knW/uRannXoip516IgC79+vPe+99kMftq4U75cRjOOnM3zFv/jx6dO3C2aecyJyPPubsP1zO1UOGMn/+fHbvvSPr91xnsefq2KE9Rx/Rn4N/dAIAPz7yEDp2aM/7M2Zy3Em/Ye68eaQFia0235gD99kj71vTYixI5ergjJTDDUfEdcANKaUn6zh2W0ppsYOBdjWrpfh0qstV9PXXZpV1cpv+fdia+zX53/ub3xrRbKer55LxppS+/GSHz445A0eStEjZsiyXE0mSVEU+q1mSVCjfTiRJknJjxitJKpQP0JAkqYqa+7rbpmZXsyRJVWTGK0kqlJOrJElSbsx4JUmFcnKVJElV5OQqSZKUGzNeSVKh8nhZT3NmxitJUhWZ8UqSClW25UQGXklSoZxcJUmScmPGK0kqVNnW8ZrxSpJURWa8kqRClW1ylRmvJElVZMYrSSpU2R6gYeCVJBXK5USSJCk3ZrySpEK5nEiSJOXGjFeSVKiyLScy8EqSClW2Wc12NUuSVEVmvJKkQpWtq9mMV5KkKjLjlSQVqmzLiQy8kqRCLXBylSRJyosZrySpUOXKd814JUmqKjNeSVKhXE4kSZJyY8YrSSpU2TJeA68kqVA+q1mSJOXGjFeSVKiydTWb8UqSSikiro+IdyPi1VplK0XE6IiYkP3slJVHRFwaERMj4uWI2LzWdwZk9SdExIDFXdfAK0kqVMrhf410I9D3C2UnAw+llHoCD2X7ALsDPbPPIOBPUAnUwBnA1sBWwBkLg3V9DLySpEKllJr808jrPg7M+ELx3sCQbHsIsE+t8ptSxRhgxYjoAuwGjE4pzUgpzQRG8+Vg/jkGXklSixMRgyLiuVqfQY38aueU0rRsezrQOdvuBkyqVW9yVlZfeb2cXCVJKlQek6tSSoOBwUt5jhQRTd44M15Jkj7zTtaFTPbz3ax8CtCjVr3uWVl95fUy8EqSClXUGG89RgELZyYPAEbWKj88m928DTAr65J+AOgTEZ2ySVV9srJ62dUsSSpUUet4I2IosBOwSkRMpjI7+QJgWEQMBN4CDsyq3wf0AyYCnwBHAqSUZkTE2cDYrN5ZKaUvTtj6/HWb66O6Wrft1jwbJi2hT6c+UXQTpKXWZpV1Iq9zb7L6dk3+9/6l6U/n1t6lZcYrSSrUEqy7bREc45UkqYrMeCVJhVrQTIc882LGK0lSFZnxSpIKVbYxXgOvJKlQdjVLkqTcmPFKkgpVtq5mM15JkqrIjFeSVKiyjfEaeCVJhbKrWZIk5caMV5JUqLJ1NZvxSpJURWa8kqRClW2M18ArSSpUSguKbkJV2dUsSVIVmfFKkgq1oGRdzWa8kiRVkRmvJKlQyeVEkiQpL2a8kqRClW2M18ArSSqUXc2SJCk3ZrySpEL5rGZJkpQbM15JUqF8VrMkSVXk5CpJkpQbM15JUqHKto7XjFeSpCoy45UkFapsY7wGXklSoVzHK0mScmPGK0kqVNm6ms14JUmqIjNeSVKhXE4kSZJyY8YrSSpU2cZ4DbySpEK5nEiSJOXGjFeSVKiyvRbQjFeSpCoy45UkFapsY7wGXklSoco2q9muZkmSqsiMV5JUKCdXSZKk3JjxSpIKVbYxXgOvJKlQZQu8djVLklRFZrySpEKVK98145UkqaqibH3r+kxEDEopDS66HdLS8ndZXydmvOU2qOgGSE3E32V9bRh4JUmqIgOvJElVZOAtN8fE1FL4u6yvDSdXSZJURWa8kiRVkYFXkqQqMvCWVET0jYh/RMTEiDi56PZIX0VEXB8R70bEq0W3RWosA28JRUQr4Apgd2BDoH9EbFhsq6Sv5Eagb9GNkJaEgbectgImppTeTCnNBW4H9i64TdISSyk9Dswouh3SkjDwllM3YFKt/clZmSQpZwZeSZKqyMBbTlOAHrX2u2dlkqScGXjLaSzQMyLWjoi2wMHAqILbJEmlYOAtoZTSfOA44AFgPDAspfRasa2SllxEDAX+DnwrIiZHxMCi2yQtjo+MlCSpisx4JUmqIgOvJElVZOCVJKmKDLySJFWRgVeSpCoy8EpARNRExLiIeDUihkfEcktxrhsjYv9s+9qGXkARETtFxHZf4Rr/iohVvmobJRXHwCtVfJpS2jSl9G1gLvDj2gcjovVXOWlK6UcppdcbqLITsMSBV9LXl4FX+rIngHWzbPSJiBgFvB4RrSLi9xExNiJejoijAaLi8uz9xn8DVlt4ooh4NCK2yLb7RsQLEfFSRDwUEWtRCfAnZtn2DhGxakTclV1jbET0yr67ckQ8GBGvRcS1QFT5/xNJTeQr/SteaqmyzHZ34K9Z0ebAt1NK/4yIQcCslNKWEfEN4KmIeBDYDPgWlXcbdwZeB67/wnlXBa4Bvpeda6WU0oyIuAr4KKX0h6zebcBFKaUnI2INKk8X2wA4A3gypXRWROwB+IQm6WvKwCtVtIuIcdn2E8B1VLqAn00p/TMr7wNsvHD8FugI9AS+BwxNKdUAUyPi4TrOvw3w+MJzpZTqe4fsrsCGEYsS2g4RsUJ2jf2y794bETO/2m1KKpqBV6r4NKW0ae2CLPh9XLsIOD6l9MAX6vVrwnYsA2yTUvp3HW2R1AI4xis13gPAMRHRBiAi1ouI5YHHgYOyMeAuwM51fHcM8L2IWDv77kpZ+Rygfa16DwLHL9yJiE2zzceBQ7Ky3YFOTXVTkqrLwCs13rVUxm9fiIhXgaup9BrdDUzIjt1E5W05n5NSeg8YBIyIiJeAO7JD9wD7LpxcBfwPsEU2eet1PptdfSaVwP0alS7nt3O6R0k58+1EkiRVkRmvJElVZOCVJKmKDLySJFWRgVeSpCoy8EqSVEUGXkmSqsjAK0lSFf1/A9qKouYoZRUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "col = [ i for i in range(0,2)] #類別名稱\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, columns=col, index = col)\n",
    "df_cm.index.name = 'True Label'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing 分布情況 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 8339\n",
      "  Batch size = 64\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.3614237308502197,\n",
       " 'test_accuracy': 0.8732461925890395,\n",
       " 'test_runtime': 5.3017,\n",
       " 'test_samples_per_second': 1572.9,\n",
       " 'test_steps_per_second': 24.709}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(test_dataset)\n",
    "print('test: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  {'accuracy': 0.8732461925890395}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "print('accuracy : ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.52      0.65      1870\n",
      "           1       0.88      0.98      0.92      6469\n",
      "\n",
      "    accuracy                           0.87      8339\n",
      "   macro avg       0.87      0.75      0.79      8339\n",
      "weighted avg       0.87      0.87      0.86      8339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for line in classification_report(labels, predictions).split('\\n'):\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 971  899]\n",
      " [ 158 6311]]\n"
     ]
    }
   ],
   "source": [
    "# **產生 confusion matrix heatmap **\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "cf_matrix = confusion_matrix(labels, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6032235729292296\n"
     ]
    }
   ],
   "source": [
    "matthews = matthews_corrcoef(labels, predictions)\n",
    "print(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Predicted', ylabel='True Label'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "col = [ i for i in range(0,2)] #類別名稱\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, columns=col, index = col)\n",
    "df_cm.index.name = 'True Label'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
