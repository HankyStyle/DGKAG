{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Transformer 來做 Triplet 的分類任務 (Positive / Negative)\n",
    "任務說明:\n",
    "給定一段句子與 triplet，判斷 triplet 是否為 positive 或是 negative\n",
    "1. Positive (0) 定義 : 與 distractor 相關的 triplet<br>\n",
    "2. Negative (1) 定義 : 與 distractor 不相關的 triplet<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  7 07:08:35 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX                On | 00000000:01:00.0 Off |                  N/A |\n",
      "| 41%   40C    P8               19W / 280W|      1MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN RTX                On | 00000000:02:00.0 Off |                  N/A |\n",
      "| 41%   35C    P8               33W / 280W|      1MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight and Bias (Assisting Metrics, Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"test Sentence Transformer reranker on Sciq\"\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = project_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 07:08:35.976008: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 07:08:36.082494: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-07 07:08:36.549053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-07 07:08:36.549169: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-07 07:08:36.549177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, sys\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(item):\n",
    "    path = '../../../../data/sciq/{}.with.triplet.json'.format(item)\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('train')\n",
    "valid = read_data('valid')\n",
    "test = read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11679, 1000, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'What type of organism is commonly used in preparation of foods such as cheese and yogurt?',\n",
       " 'distractors': ['protozoa', 'gymnosperms', 'viruses'],\n",
       " 'answer': 'mesophilic organisms',\n",
       " 'triplets': [['isa', 'yogurt', 'food', 0.5036563873291016],\n",
       "  ['relatedto', 'cheese', 'yogurt', 0.5036563873291016],\n",
       "  ['relatedto', 'organisms', 'organism', 0.48464348912239075],\n",
       "  ['antonym', 'thermophilic', 'psychrophilic', 0.3640008568763733],\n",
       "  ['isa', 'cheese', 'food', 0.33733850717544556],\n",
       "  ['relatedto', 'preparation', 'food', 0.33733850717544556],\n",
       "  ['relatedto', 'cheese', 'food', 0.33733850717544556],\n",
       "  ['relatedto', 'foods', 'food', 0.33733850717544556],\n",
       "  ['relatedto', 'food', 'foods', 0.33733850717544556],\n",
       "  ['antonym', 'anaerobic', 'aerobic', 0.28718218207359314],\n",
       "  ['antonym', 'aerobic', 'anaerobic', 0.28718218207359314],\n",
       "  ['relatedto', 'anaerobic', 'organism', 0.28718218207359314],\n",
       "  ['relatedto', 'used', 'use', 0.14599347114562988]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compounds that are capable of accepting electrons, such as o 2 or f2, are called what?\n",
      "['antioxidants', 'Oxygen', 'residues']\n",
      "oxidants\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['relatedto', 'oxidants', 'oxidant', 0.6359457969665527],\n",
       " ['relatedto', 'electrons', 'electron', 0.46036529541015625],\n",
       " ['relatedto', 'delocalized', 'electron', 0.43334996700286865],\n",
       " ['relatedto', 'redox', 'electron', 0.43334996700286865],\n",
       " ['relatedto', 'proton', 'electron', 0.43334996700286865],\n",
       " ['relatedto', 'electron', 'proton', 0.43334996700286865],\n",
       " ['relatedto', 'delocalized', 'compound', 0.32832592725753784],\n",
       " ['relatedto', 'called', 'call', -0.034704722464084625]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Triplet with Sentence Transformer\n",
    "print(test[0]['sentence'])\n",
    "print(test[0]['distractors'])\n",
    "print(test[0]['answer'])\n",
    "test[0]['triplets'][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Relation_Dict ={\n",
    " 'antonym': 'is the antonym of',\n",
    " 'atlocation' : 'is at location of',\n",
    " 'capableof': 'is capable of',\n",
    " 'causes' : 'causes',\n",
    " 'createdby': 'is created by',\n",
    " 'desires': 'desires',\n",
    " 'hasproperty': 'has property',\n",
    " 'hassubevent': 'has subevent',\n",
    " 'isa':'is a kind of',\n",
    " 'madeof':'is made of',\n",
    " 'notcapableof':'has not capable of',\n",
    " 'notdesires': \"does not desires\",\n",
    " 'partof':'is part of',\n",
    " 'receivesaction':'is',\n",
    " 'relatedto':'is related to',\n",
    " 'usedfor':'is used for'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data):\n",
    "    \n",
    "    sentences = []\n",
    "    triplets = []\n",
    "    labels = []\n",
    "    answers = []\n",
    "    for d in data:\n",
    "\n",
    "        sentence = d['sentence']\n",
    "        distractors = d['distractors']\n",
    "        answer = d['answer']\n",
    "        triplet = d['triplets']\n",
    "\n",
    "\n",
    "        distractors = [dis.strip() for dis in distractors]\n",
    "        sentence = sentence + ' ' +answer\n",
    "\n",
    "        \n",
    "        for each_triplet in triplet:\n",
    "            rel, source, target, weight = each_triplet\n",
    "\n",
    "            sentences.append(sentence)\n",
    "            triplets.append('{} {} {}'.format(source,Relation_Dict[rel],target))\n",
    "\n",
    "            if source == answer or target == answer or source in distractors or target in distractors:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "            \n",
    "              \n",
    "        \n",
    "    return sentences, triplets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent, train_triplet, train_label = processData(train)\n",
    "valid_sent, valid_triplet, valid_label = processData(valid)\n",
    "test_sent, test_triplet, test_label = processData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260474, 22596, 22403)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sent), len(valid_sent), len(test_sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "統計訓練與測試資料分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 資料分布 : Positive 有 39688 筆, Negative 有 220786 筆，大約有 15.24% 為 Positive。\n",
      "Valid 資料分布 : Positive 有 3147 筆, Negative 有 19449 筆，大約有 13.93% 為 Positive。\n",
      "Test 資料分布 : Positive 有 3499 筆, Negative 有 18904 筆，大約有 15.62% 為 Positive。\n"
     ]
    }
   ],
   "source": [
    "print('Train 資料分布 : Positive 有 {} 筆, Negative 有 {} 筆，大約有 {:.2f}% 為 Positive。'.format(train_label.count(0),train_label.count(1),train_label.count(0)/len(train_sent)*100))\n",
    "print('Valid 資料分布 : Positive 有 {} 筆, Negative 有 {} 筆，大約有 {:.2f}% 為 Positive。'.format(valid_label.count(0),valid_label.count(1),valid_label.count(0)/len(valid_sent)*100))\n",
    "print('Test 資料分布 : Positive 有 {} 筆, Negative 有 {} 筆，大約有 {:.2f}% 為 Positive。'.format(test_label.count(0),test_label.count(1),test_label.count(0)/len(test_sent)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_sent, train_triplet, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_sent, valid_triplet, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_sent, test_triplet, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2058, 2832, 2001, 15927, 2007, 4145, 2113, 2003, 7551, 2001, 9444, 2111, 2008, 8812, 2002, 10934, 27394, 2106, 1033, 2037, 28797, 19470, 2598, 11771, 2, 2, 10934, 27394, 2106, 2007, 1041, 2789, 2001, 2837, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<s> what type of organism is commonly used in preparation of foods such as cheese and yogurt? mesophilic organisms </s> </s> yogurt is a kind of food </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'][0])\n",
    "print(tokenizer.decode(train_encodings['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(encodings, label):\n",
    "    encodings.update({'labels': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_labels(train_encodings, train_label)\n",
    "add_labels(valid_encodings, valid_label)\n",
    "add_labels(test_encodings, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['labels'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義 Dataset，並轉換成 tensor 格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings):\n",
    "    self.encodings = encodings\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = Dataset(train_encodings)\n",
    "valid_dataset = Dataset(valid_encodings)\n",
    "test_dataset = Dataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0,  2058,  2832,  2001, 15927,  2007,  4145,  2113,  2003,  7551,\n",
       "          2001,  9444,  2111,  2008,  8812,  2002, 10934, 27394,  2106,  1033,\n",
       "          2037, 28797, 19470,  2598, 11771,     2,     2, 10934, 27394,  2106,\n",
       "          2007,  1041,  2789,  2001,  2837,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentence-Transformers model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/all-mpnet-base-v2 were not used when initializing MPNetForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MPNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MPNetConfig, MPNetForSequenceClassification\n",
    "config  = MPNetConfig.from_pretrained('sentence-transformers/all-mpnet-base-v2', num_labels = 2) # num_labels 設定類別數\n",
    "model = MPNetForSequenceClassification.from_pretrained('sentence-transformers/all-mpnet-base-v2', config=config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看模型架構"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNetForSequenceClassification(\n",
      "  (mpnet): MPNetModel(\n",
      "    (embeddings): MPNetEmbeddings(\n",
      "      (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): MPNetEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): MPNetLayer(\n",
      "          (attention): MPNetAttention(\n",
      "            (attn): MPNetSelfAttention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (intermediate): MPNetIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): MPNetOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (relative_attention_bias): Embedding(32, 12)\n",
      "    )\n",
      "  )\n",
      "  (classifier): MPNetClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "   \n",
    "    results = metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {'accuracy': results['accuracy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    weight_decay=0.01,\n",
    "    eval_accumulation_steps = 1,\n",
    "    report_to=\"wandb\" if os.getenv(\"WANDB_PROJECT\") else \"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 260474\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 40700\n",
      "  Number of trainable parameters = 109488002\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhankystyle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/user_data/DG-via-Text2Text-with-Triplet-Augmentation/reranker/wandb/run-20230707_141134-b21y0o1s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hankystyle/test%20Sentence%20Transformer%20reranker%20on%20Sciq/runs/b21y0o1s' target=\"_blank\">lilac-mountain-2</a></strong> to <a href='https://wandb.ai/hankystyle/test%20Sentence%20Transformer%20reranker%20on%20Sciq' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hankystyle/test%20Sentence%20Transformer%20reranker%20on%20Sciq' target=\"_blank\">https://wandb.ai/hankystyle/test%20Sentence%20Transformer%20reranker%20on%20Sciq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hankystyle/test%20Sentence%20Transformer%20reranker%20on%20Sciq/runs/b21y0o1s' target=\"_blank\">https://wandb.ai/hankystyle/test%20Sentence%20Transformer%20reranker%20on%20Sciq/runs/b21y0o1s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a MPNetTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40700' max='40700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40700/40700 3:32:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.149352</td>\n",
       "      <td>0.950478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.083100</td>\n",
       "      <td>0.192110</td>\n",
       "      <td>0.938042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.219127</td>\n",
       "      <td>0.944902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.270437</td>\n",
       "      <td>0.942246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.282756</td>\n",
       "      <td>0.942999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.386671</td>\n",
       "      <td>0.940742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.406145</td>\n",
       "      <td>0.932820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.367543</td>\n",
       "      <td>0.939016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.411724</td>\n",
       "      <td>0.939547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.440460</td>\n",
       "      <td>0.938440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-4070\n",
      "Configuration saved in ./results/checkpoint-4070/config.json\n",
      "Model weights saved in ./results/checkpoint-4070/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4070/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4070/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-5416] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-8140\n",
      "Configuration saved in ./results/checkpoint-8140/config.json\n",
      "Model weights saved in ./results/checkpoint-8140/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-8140/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8140/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-6770] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-12210\n",
      "Configuration saved in ./results/checkpoint-12210/config.json\n",
      "Model weights saved in ./results/checkpoint-12210/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-12210/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12210/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-8140] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-16280\n",
      "Configuration saved in ./results/checkpoint-16280/config.json\n",
      "Model weights saved in ./results/checkpoint-16280/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-16280/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16280/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-12210] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-20350\n",
      "Configuration saved in ./results/checkpoint-20350/config.json\n",
      "Model weights saved in ./results/checkpoint-20350/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-20350/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-20350/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-16280] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-24420\n",
      "Configuration saved in ./results/checkpoint-24420/config.json\n",
      "Model weights saved in ./results/checkpoint-24420/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-24420/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-24420/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-20350] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-28490\n",
      "Configuration saved in ./results/checkpoint-28490/config.json\n",
      "Model weights saved in ./results/checkpoint-28490/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-28490/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-28490/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-24420] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-32560\n",
      "Configuration saved in ./results/checkpoint-32560/config.json\n",
      "Model weights saved in ./results/checkpoint-32560/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-32560/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-32560/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-28490] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-36630\n",
      "Configuration saved in ./results/checkpoint-36630/config.json\n",
      "Model weights saved in ./results/checkpoint-36630/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-36630/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-36630/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-32560] due to args.save_total_limit\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-40700\n",
      "Configuration saved in ./results/checkpoint-40700/config.json\n",
      "Model weights saved in ./results/checkpoint-40700/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-40700/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-40700/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-36630] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-4070 (score: 0.950477960701009).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40700, training_loss=0.050386017587319636, metrics={'train_runtime': 12746.8325, 'train_samples_per_second': 204.344, 'train_steps_per_second': 3.193, 'total_flos': 1.6464123928051555e+17, 'train_loss': 0.050386017587319636, 'epoch': 10.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 22554\n",
      "  Batch size = 64\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56/353 00:04 < 00:22, 13.07 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.15615184605121613,\n",
       " 'eval_accuracy': 0.9524252904141173,\n",
       " 'eval_runtime': 26.965,\n",
       " 'eval_samples_per_second': 836.417,\n",
       " 'eval_steps_per_second': 13.091,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 22403\n",
      "  Batch size = 64\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.1710093915462494,\n",
       " 'test_accuracy': 0.9469267508815784,\n",
       " 'test_runtime': 22.0051,\n",
       " 'test_samples_per_second': 1018.083,\n",
       " 'test_steps_per_second': 15.951}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(test_dataset)\n",
    "print('test: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/sentence-transformer-sciq-all\n",
      "Configuration saved in ./saved_models/sentence-transformer-sciq-all/config.json\n",
      "Model weights saved in ./saved_models/sentence-transformer-sciq-all/pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/sentence-transformer-sciq-all/tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/sentence-transformer-sciq-all/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./saved_models/sentence-transformer-sciq-all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence =  Compounds that are capable of accepting electrons, such as o 2 or f2, are called what? oxidants\n",
      "triplet =  oxidants is related to oxidant\n"
     ]
    }
   ],
   "source": [
    "sentence = test_sent[0]\n",
    "triplet = test_triplet[0]\n",
    "print('sentence = ',sentence)\n",
    "print('triplet = ',triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./saved_models/sentence-transformer-sciq-all/config.json\n",
      "Model config MPNetConfig {\n",
      "  \"_name_or_path\": \"sentence-transformers/all-mpnet-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"MPNetForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"mpnet\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 30527\n",
      "}\n",
      "\n",
      "loading weights file ./saved_models/sentence-transformer-sciq-all/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MPNetForSequenceClassification.\n",
      "\n",
      "All the weights of MPNetForSequenceClassification were initialized from the model checkpoint at ./saved_models/sentence-transformer-sciq-all.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MPNetForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MPNetConfig, MPNetForSequenceClassification\n",
    "config  = MPNetConfig.from_pretrained('./saved_models/sentence-transformer-sciq-all', num_labels = 2)\n",
    "model = MPNetForSequenceClassification.from_pretrained('./saved_models/sentence-transformer-sciq-all', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encodings = tokenizer([sentence], [triplet], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = Dataset(input_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    0, 10103,  2012,  2028,  5218,  2001, 10568, 15061,  1014,  2111,\n",
       "          2008,  1055,  1020,  2034,  1046,  2479,  1014,  2028,  2174,  2058,\n",
       "          1033, 23064,  8528,  7670,     2,     2, 23064,  8528,  7670,  2007,\n",
       "          3145,  2004, 23064,  8528,  3376,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation 分布情況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 22596\n",
      "  Batch size = 64\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.14935167133808136,\n",
       " 'test_accuracy': 0.950477960701009,\n",
       " 'test_runtime': 21.7443,\n",
       " 'test_samples_per_second': 1039.171,\n",
       " 'test_steps_per_second': 16.28}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(valid_dataset)\n",
    "print('valid: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6427/2714362684.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  {'accuracy': 0.950477960701009}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "print('accuracy : ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81      3147\n",
      "           1       0.96      0.98      0.97     19449\n",
      "\n",
      "    accuracy                           0.95     22596\n",
      "   macro avg       0.91      0.88      0.89     22596\n",
      "weighted avg       0.95      0.95      0.95     22596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for line in classification_report(labels, predictions).split('\\n'):\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2454   693]\n",
      " [  426 19023]]\n"
     ]
    }
   ],
   "source": [
    "# **產生 confusion matrix heatmap **\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "cf_matrix = confusion_matrix(labels, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7868568412643547\n"
     ]
    }
   ],
   "source": [
    "matthews = matthews_corrcoef(labels, predictions)\n",
    "print(matthews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Predicted', ylabel='True Label'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAHgCAYAAACb/XXRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAr6ElEQVR4nO3dd7gV1bmA8fej2bEXBBQL1niDStQbY65drIgahUTFijWJ6Ro1lmg0tiQmlmCCYixornrFEoXYjVFRgwWUgGgUPIKKihWBs+4fe9h7I6cBZzMD5/35zMPsNW1tcnI+vm+tmYmUEpIkKX/t8u6AJEkqMShLklQQBmVJkgrCoCxJUkEYlCVJKgiDsiRJBdEh7w40ZtM1tvFeLS0RJk6vy7sL0kKb8fmbUatzz3x3Yqv/vu+42vo1628tmSlLklQQhc2UJUltRP3svHtQGAZlSVK+Un3ePSgMy9eSJBWEmbIkKV/1ZspzmClLklQQZsqSpFwlx5TLDMqSpHxZvi6zfC1JUkGYKUuS8mX5usxMWZKkgjBTliTlyyd6lZkpS5JUEGbKkqR8OaZcZlCWJOXLW6LKLF9LklQQZsqSpFz5RK8KM2VJkgrCTFmSlC/HlMsMypKkfFm+LrN8LUlSQZgpS5Ly5RO9ysyUJUkqCDNlSVK+HFMuMyhLkvLl7Osyy9eSJBWEmbIkKV+Wr8vMlCVJKggzZUlSvhxTLjMoS5JylZL3Kc9h+VqSpIIwKEuS8pXqW39pgYgYEhFTI+KlqrZbImJ0trweEaOz9h4R8VnVtqurjtk6Il6MiAkRcXlERNa+SkSMjIjx2Z8rN9cng7Ikqa26DuhT3ZBSOiSl1Cul1Au4Dbi9avOrc7allI6var8KOBbomS1zznkq8EBKqSfwQPa5SQZlSVK+6utbf2mBlNKjwLSGtmXZ7sHAzU2dIyK6AJ1TSk+mlBJwPbB/trkvMDRbH1rV3iiDsiRJ89oBmJJSGl/Vtl5E/CsiHomIHbK2rsCkqn0mZW0Aa6aU6rL1t4E1m7uos68lSfmqwcNDImIQMKiqaXBKafB8nGIAc2fJdcA6KaX3ImJr4P8iYvOWniyllCIiNbefQVmSlK8avLoxC8DzE4TLIqIDcACwddX5ZgAzsvVnI+JVYCNgMtCt6vBuWRvAlIjoklKqy8rcU5u7tuVrSZLmtivwSkqpXJaOiNUjon22vj6lCV0Ts/L09IjYLhuHPhy4MztsODAwWx9Y1d4og7IkKV/53RJ1M/BPYOOImBQRR2eb+jPvBK9vAi9kt0j9L3B8SmnOJLETgT8BE4BXgb9l7RcCu0XEeEqB/sJm+1SaLFY8m66xTTE7Js2nidPrmt9JKrgZn78ZtTr350//tdV/3y+9zbdq1t9ackxZkpQvn31dZlCWJOXLVzeWOaYsSVJBmClLkvJl+brMTFmSpIIwU5Yk5ctMucygLEnKVUqt/0SvxZXla0mSCsJMWZKUL8vXZWbKkiQVhJmyJClfPjykzExZkqSCMFOWJOXLMeUyg7IkKV+Wr8ssX0uSVBBmypKkfFm+LjNTliSpIMyUJUn5cky5zKAsScqX5esyy9eSJBWEmbIkKV9mymVmypIkFYSZsiQpX070KjMoS5LyZfm6zPK1JEkFYaYsScqX5esyM2VJkgrCTFmSlC/HlMvMlCVJKggzZUlSvhxTLjMoS5LyZfm6zPK1JEkFYaYsScqXmXKZmbIkSQVhpixJyldKefegMAzKkqR8Wb4us3wtSVJBmClLkvJlplxmpixJUkGYKUuS8uUTvcoMypKkfFm+LrN8LUlSQZgpS5Ly5X3KZWbKkiQVhJmyJClfjimXmSlLklQQZsqSpHyZKZcZlCVJ+fI+5TLL15IkFYRBWZKUq1SfWn1piYgYEhFTI+KlqrazI2JyRIzOlr2qtp0WERMiYlxE7FHV3idrmxARp1a1rxcRT2Xtt0REp+b6ZFCWJLVV1wF9Gmj/TUqpV7bcCxARmwH9gc2zY66MiPYR0R64AtgT2AwYkO0L8OvsXBsC7wNHN9chg7IkKV/19a2/tEBK6VFgWgt72RcYllKakVJ6DZgAbJMtE1JKE1NKXwDDgL4REcDOwP9mxw8F9m/uIgZlSVK+Un3rLwvn5Ih4IStvr5y1dQXerNpnUtbWWPuqwAcppVlfam+SQVmStMSJiEER8UzVMqiFh14FbAD0AuqAS2vVx4Z4S5QkKV8tnJg1P1JKg4HBC3DclDnrEXENcHf2cTLQvWrXblkbjbS/B6wUER2ybLl6/0aZKUuSlImILlUf+wFzZmYPB/pHxFIRsR7QE3gaGAX0zGZad6I0GWx4SikBDwEHZccPBO5s7vpmypKkfOX0RK+IuBnYEVgtIiYBZwE7RkQvIAGvA8cBpJTGRMStwFhgFnBSSml2dp6TgfuB9sCQlNKY7BI/A4ZFxHnAv4A/N9cng7IkKV85BeWU0oAGmhsNnCml84HzG2i/F7i3gfaJlGZnt5jla0mSCsJMWZKUr9T6E70WV2bKkiQVhJmyJClfvrqxzExZkqSCMCgvRtZaew2uu/1K7npsGHc9OozDjj1knn2+9vWteHrCg9z+4A3c/uANnPijZp9/3qyOnTpy2eDzue+p2xj2tyGs3b10G98WW25Wvs4dD93IrnvtuNDXUtuw4oqdufmmq3nh+Yd4fvSDbLvtVnNtX2mlFbn1lmt4ZtQIHn/sLjbbbOOFvmanTp244S9XMnbMYzz26HDWXbcbALvssgP/fOIenn1mJP984h523PHrC30tzaf61PrLYsry9WJk9qzZXHTW7xj74jiWXW5Zbvv79TzxyNO8+u/X5trv2SdHc8KhP5zv86/dvQsXXP4LBvY7Ya72g76zHx9++BF9tj2QvfbfjR+feTI/HHQ64195lW/tNpDZs2ez+hqrcsdDN/LQ/Y8xe/bshfqeWvJdeunZjBj5MAO+fTwdO3Zk2WWXmWv7z356Ms+/MIaDDzmWjTfagN/97jz67NnQ3SvzWnfdblxzzWXsvvvBc7UfeUR/PvjgAzbbfAe+9a39OP+8n3PoYSfy7rvTOODAo6irm8Jmm23M3XfdwPobfK3VvqtaYOGfVb3EMFNejLwz9T3GvjgOgE8/+ZRX//0aa3ZZvcXH73tQH26571puf/AGzr7kVNq1a9n//Dv3+R/uvOUeAO6/60G226H0C+vzz2aUA3CnpZcisfj+61SLTufOK7DDN7bl2muHATBz5kw+/HD6XPtsumlPHn74CQDG/ftV1l23O2ussRoAAwb04/HH7uLpp+7jij9c0OKf43333Z2/3FB6Yc/tt9/DTjttD8Dzz4+hrq70ZMWxY8exzDJL06lTs6+9lWqiZkE5IjaJiJ9FxOXZ8rOI2LRW12tr1u7ehU232Jjnnx0zz7Zevbfgjodu5I83/5YNN14fgPV79mDPvrvxnX2O4YCdD6V+dj37HtTQa0TnteZaq1M3ufRLa/bs2Xz00cestMqKAPzXVptz16PDuPORmzjnJ782S1azevTozjvvTOOaay7jqSf/xlVXXTRPpvzCiy+zf989AejduxfrrNOVrl27sMnGG/Ktg/Zlx536sc22fZg9u54BA/q16Lprr70Wkya9BZR+jqdP/4hVV115rn369duL0aNf5IsvvmiFb6oWs3xdVpPydUT8DBhA6b2ST2fN3YCbI2JYSunCWly3rVh2uWW4fMiFXHjmZXzy8SdzbRv7wjh22Xo/Pv3kM765y9f5w9CL6LPdQWy3w9fY/KubcOuIoQAsvfRSvPfu+wD8/rqL6LrO2nTs2IEu3dbi9gdvAOAvg4dxx7C7acoLz41h32/2Z/2ePbjg92fx6ANP8MUMf6GpcR06dGDLLb/CD354JqNGjebSS87mJz85iXPOuaS8z8UXX8Gll57D00/dx0tjXmH06DHMnj2bnXbani23/C+e+Efp53KZZZZm6jvvAnDrLdfQo0d3OnXqSPfuXXn6qfsA+MMVQ7j++lub7demm27Er87/OXvv850afGupZWo1pnw0sHlKaWZ1Y0RcBowBGgzK2au1BgGstfy6rLTMGjXq3uKrQ4f2/G7Ir7nrtvsZec/D82yvDtKPPvAEv/j1T1lplRWJCP7vlnv4zflXznPMd4/4KdD4mPKUt9+hS9c1mVI3lfbt27PCCsvzwbQP59pn4vjX+fSTz+i5yQaMef7lVvimWlJNnlzHpMl1jBo1GoDb77iXn/z4xLn2+eijjxk06Eflz+PGPcFrr73BN7bfhhtu/Ctnnvnrec578CHHAo2PKb/11tt067Y2kye/Tfv27enceQXee6/0D9OuXdfir7dew1FHn8LEif9pza+rFkjeElVWq/J1PbB2A+1dsm0NSikNTin1Tin1NiA37LzfnsnEf7/G0KtvanD7amusWl7fYsvNiHbt+GDahzz52Cj22HdnVlmtVK5bcaXOrN1trRZd86H7H6XvIXsDsMe+O/Pk488A0HWdtWnfvj0Aa3dbi/V7rsvkN99a4O+mtmHKlHeYNKmOjXqWhlZ22ml7Xn55/Fz7rLhiZzp27AjAUUcN4PHHn+Kjjz7mwYf+wQH99mb11Us/5yuvvBLrrNPse+MBuPvukRx2aOmFPQccsDcPP/yP8rX+746hnH7GBfzzn8+0ynfUfLJ8XVarTPkU4IGIGA+8mbWtA2wInFyjay7xttr2q/Q9eC/GjR1fLjH/9vwr6ZIF11uG3s7u++zMgCMOZNbs2cz47HN+dNzpALz679f43QVX86dbf0+7dsGsmbP45akX89akt5u97v/eOJxfX3EO9z11Gx++P718zq23/SrHfncgM2fNItXXc+7PLpong5Ya8oMfnMl11/2eTp068tprb3DsoB9x7DGHAnDNn25gk0025M9/+g0pJcaO/TfHHf8TAF55ZTxnnX0x99x9I+3atWPmzJl8/5QzeOONZl9Ty7XXDePaIb9l7JjHmDbtAw47/CQATjjhCDbYoAen//wUTv/5KQDsvc93eOed92rz5aUmRKrRM0cjoh2lt2PM+WfsZGDUnFddNWfTNbZZfP+pI1WZOL0u7y5IC23G529Grc79yXmHtvrv++XOuKFm/a2lmt2nnFKqB56s1fklSVrS+PAQSVK+FuMx4NZmUJYk5cvZ12U+0UuSpIIwU5Yk5cvydZmZsiRJBWGmLEnKl2+JKjNTliSpIMyUJUn5cky5zKAsScqVL6SosHwtSVJBmClLkvJl+brMTFmSpIIwU5Yk5ctMucygLEnKl/cpl1m+liSpIMyUJUn5snxdZqYsSVJBmClLknKVzJTLDMqSpHwZlMssX0uSVBBmypKkfPns6zIzZUmSCsJMWZKUL8eUy8yUJUkqCDNlSVK+zJTLDMqSpFylZFCew/K1JEkFYaYsScqX5esyM2VJkgrCTFmSlC8z5TKDsiQpV76QosLytSRJBWGmLEnKl5lymZmyJEkFYaYsScqXL4kqM1OWJOUq1adWX1oiIoZExNSIeKmq7eKIeCUiXoiIOyJipay9R0R8FhGjs+XqqmO2jogXI2JCRFweEZG1rxIRIyNifPbnys31yaAsSWqrrgP6fKltJPCVlNJ/Af8GTqva9mpKqVe2HF/VfhVwLNAzW+ac81TggZRST+CB7HOTDMqSpHzVp9ZfWiCl9Cgw7UttI1JKs7KPTwLdmjpHRHQBOqeUnkylh3hfD+yfbe4LDM3Wh1a1N8qgLEla4kTEoIh4pmoZtACnOQr4W9Xn9SLiXxHxSETskLV1BSZV7TMpawNYM6VUl62/DazZ3AWd6CVJylcNJnqllAYDgxf0+Ig4HZgF3Jg11QHrpJTei4itgf+LiM3noz8pIppN4Q3KkiRViYgjgH2AXbKSNCmlGcCMbP3ZiHgV2AiYzNwl7m5ZG8CUiOiSUqrLytxTm7u25WtJUq7ymn3dkIjoA/wU2C+l9GlV++oR0T5bX5/ShK6JWXl6ekRsl826Phy4MztsODAwWx9Y1d4oM2VJUr5yuk85Im4GdgRWi4hJwFmUZlsvBYzM7mx6Mptp/U3g3IiYmfX4+JTSnEliJ1Kayb0MpTHoOePQFwK3RsTRwH+Ag5vrk0FZktQmpZQGNND850b2vQ24rZFtzwBfaaD9PWCX+emTQVmSlCvfElXhmLIkSQVhpixJypfPvi4zKEuScpUMymWWryVJKggzZUlSvsyUy8yUJUkqCDNlSVKuHFOuMChLkvJlUC6zfC1JUkGYKUuScmX5usJMWZKkgjBTliTlyky5wqAsScqVQbnC8rUkSQVhpixJyleKvHtQGGbKkiQVhJmyJClXjilXmClLklQQZsqSpFyleseU5zAoS5JyZfm6wvK1JEkFYaYsScpV8paoMjNlSZIKwkxZkpQrx5QrDMqSpFw5+7rC8rUkSQVhpixJylVKefegOMyUJUkqCDNlSVKuHFOuMChLknJlUK6wfC1JUkGYKUuScuVErwozZUmSCsJMWZKUK8eUKxoNyhGxVVMHppSea/3uSJLUdjWVKV/axLYE7NzKfZEktUG+Jaqi0aCcUtppUXZEktQ2+UKKimYnekXEshFxRkQMzj73jIh9at81SZLalpbMvr4W+AL4evZ5MnBezXokSWpT6lO0+rK4aklQ3iCldBEwEyCl9Cmw+H5jSZIKqiW3RH0REctQmtxFRGwAzKhpryRJbYYTvSpaEpTPAu4DukfEjcD2wBG17JQkqe3wPuWKZoNySmlkRDwHbEepbP39lNK7Ne+ZJEltTEuf6PU/wDcolbA7AnfUrEeSpDbFZ19XtOSWqCuB44EXgZeA4yLiilp3TJKktqYlmfLOwKYppTkTvYYCY2raK0lSm+GYckVLgvIEYB3gP9nn7lmbJEkLbXG+r7i1NfVCirsojSGvALwcEU9nn7cFnl403ZMkqe1oKlO+ZJH1QpLUZuV1n3JEDAH2AaamlL6Sta0C3AL0AF4HDk4pvR8RAfwO2Av4FDhiztsSI2IgcEZ22vNSSkOz9q2B64BlgHsp3b3U5LS2Rid6pZQeaWpZoL8BSZKK4zqgz5faTgUeSCn1BB7IPgPsCfTMlkHAVVAO4mdRqiJvA5wVEStnx1wFHFt13JevNY+WzL7eLiJGRcTHEfFFRMyOiOnNHSdJUkuk1PpLy66bHgWmfam5LzA0Wx8K7F/Vfn0qeRJYKSK6AHsAI1NK01JK7wMjgT7Zts4ppSez7Pj6qnM1qiXPvv4DMAAYTykFPwbwlihJ0pJozZRSXbb+NrBmtt4VeLNqv0lZW1Ptkxpob1JLgjIppQlA+5TS7JTStbQgBZckqSVq8ZaoiBgUEc9ULYPmt19ZhrtIH23SkluiPo2ITsDoiLgIqKOFwVySpObUYqJXSmkwMHgBDp0SEV1SSnVZCXpq1j6Z0i3Bc3TL2iYDO36p/eGsvVsD+zepJcH1sGy/k4FPsk4d0ILjJEla3AwHBmbrA4E7q9oPj5LtgA+zMvf9wO4RsXI2wWt34P5s2/RsXlYAh1edq1EteSHFnIeGfA6cAxARtwCHtPQbSpLUmLyefR0RN1PKcleLiEmUZlFfCNwaEUdTemjWwdnu91K6HWoCpVuijgRIKU2LiF8Co7L9zk0pzZk8diKVW6L+li1N96mZW6Ya+yJvpJTWme8D58Oma2zjI8q1RJg4va75naSCm/H5mzW7mfi57n1b/ff9Vm/euVg+Jqylb4mSJKkmfMxmRVOP2dyqsU2UXt9YU+M/aHY8XFosfPbWY3l3QSq0vJ7oVURNZcqXNrHtldbuiCRJbV2jQTmltNOi7IgkqW2yfF3h/caSJBWEE70kSbnyVpsKg7IkKVeWryta8paoiIhDI+IX2ed1ImKb2ndNkqS2pSVjylcC/03pTVEAH+FboiRJrSSlaPVlcdWS8vW2KaWtIuJfACml97MXVEiSpFbUkqA8MyLak43FR8TqQH1NeyVJajMMKBUtKV9fDtwBrBER5wOPA7+qaa8kSWqDWvKWqBsj4llgF0qP2Nw/pfRyzXsmSWoTEovvGHBrazYoR8Q6lF5TdVd1W0rpjVp2TJLUNtR7o3JZS8aU76E0nhzA0sB6wDhg8xr2S5KkNqcl5estqj9nb486sWY9kiS1KfWWr8vm+9nXKaXngG1r0BdJktq0lowp/7DqYztgK+CtmvVIktSmONGroiVjyitUrc+iNMZ8W226I0lqa7xPuaLJoJw9NGSFlNKPF1F/JElqsxoNyhHRIaU0KyK2X5QdkiS1LZavK5rKlJ+mNH48OiKGA38FPpmzMaV0e437JklSm9KSMeWlgfeAnancr5wAg7IkaaE5plzRVFBeI5t5/RKVYDyHz1+RJLUKg3JFU0G5PbA8NFjsNyhLktTKmgrKdSmlcxdZTyRJbZITvSqaeqKXf0uSJC1CTWXKuyyyXkiS2qx6U8CyRjPllNK0RdkRSZLaupbcEiVJUs34lqgKg7IkKVfezlMx369ulCRJtWGmLEnKlQ8PqTBTliSpIMyUJUm5qg8nes1hUJYk5cqJXhWWryVJKggzZUlSrpzoVWGmLElSQZgpS5Jy5bOvKwzKkqRc+ZjNCsvXkiQVhJmyJClX3hJVYaYsSVJBmClLknLlRK8KM2VJkgrCTFmSlCsfHlJhUJYk5cqJXhWWryVJKggzZUlSrpzoVWGmLElqcyJi44gYXbVMj4hTIuLsiJhc1b5X1TGnRcSEiBgXEXtUtffJ2iZExKkL0y8zZUlSrvKY6JVSGgf0AoiI9sBk4A7gSOA3KaVLqvePiM2A/sDmwNrA3yNio2zzFcBuwCRgVEQMTymNXZB+GZQlSbkqwOzrXYBXU0r/iWi0lt4XGJZSmgG8FhETgG2ybRNSShMBImJYtu8CBWXL15KkJU5EDIqIZ6qWQU3s3h+4uerzyRHxQkQMiYiVs7auwJtV+0zK2hprXyAGZUlSrlLUYElpcEqpd9UyuKFrR0QnYD/gr1nTVcAGlErbdcCli+LvYA7L15KktmxP4LmU0hSAOX8CRMQ1wN3Zx8lA96rjumVtNNE+38yUJUm5qq/BMh8GUFW6joguVdv6AS9l68OB/hGxVESsB/QEngZGAT0jYr0s6+6f7btAzJQlSbnKa6JXRCxHadb0cVXNF0VEL0oPGnt9zraU0piIuJXSBK5ZwEkppdnZeU4G7gfaA0NSSmMWtE8GZUlSm5RS+gRY9UtthzWx//nA+Q203wvc2xp9MihLknLls68rHFOWJKkgzJQlSbny2dcVZsqSJBWEmbIkKVcFeMxmYRiUJUm5MihXWL6WJKkgzJQlSbnylqgKM2VJkgrCTFmSlCtviaowKEuScuVErwrL15IkFYSZsiQpV070qjBTliSpIMyUJUm5qjdXLjMoS5Jy5USvCsvXkiQVhJmyJClXFq8rzJQlSSoIM2VJUq4cU64wU5YkqSDMlCVJufLZ1xUGZUlSrrxPucLytSRJBWGmLEnKlXlyhZmyJEkFYaYsScqVt0RVGJQlSblyoleF5WtJkgrCTFmSlCvz5AozZUmSCsJMWZKUKyd6VRiUJUm5cqJXheVrSZIKwkxZkpQr8+QKM2VJkgrCTFmSlCsnelUYlCVJuUoWsMssX0uSVBBmypKkXFm+rjBTliSpIMyUJUm58uEhFWbKkiQVhJmyJClX5skVBmVJUq4sX1dYvpYkqSAMyouZdu3aMerp+7nzjqHzbBt07GH867m/88yoETzy0B1sumnPhb5ejx7deeLxu3hl7OPcdONVdOzYEYBTvj+IF55/iOeeHcmI+25hnXW6LvS11Hac8avL+Obe/dn/0OMb3P7h9I/43mnn0u/wE+h/zPcZP/H1hb7mF198wY/OvIA9Dz6KAceewuS6KXNtr3t7Kl/btR/X3vS/C30tzZ/6GiyLK4PyYuZ73z2GV14Z3+C2m4fdwZZb7Urvr+3OxZdeySUXndXi8x5+2MH84swfztN+wa9O57eXX8Mmm32D99//kKOOHADA6NEvse12e7LV1rtx2+33cOEFZyzYF1KbtP9eu3H1Zec1uv2a629hk54bcMf1V/GrM3/Mhb+9usXnnlw3hSNO/uk87bffPYLOKyzP324dwmGH7M9lVw6Za/tFvx/MDtv1bvmXkGrAoLwY6dq1C3vtuQtDhtzc4PaPPvq4vL7ccsuSUmmcpl27dvz6gjP45xP38NyzIzn2mENbfM2ddtye2267B4C//OWv9N1vDwAefuQJPvvscwCeevpZunXtskDfSW1T715bsGLnFRrd/urrb7DtVl8FYP11uzO5bgrvTnsfgLvuf5D+x3yfAweexDkXXc7s2bNbdM0HH/snfffaFYDdd9yBp54dXf7/yAOPPkHXLmuxwXrrLszX0gJKNfhvcWVQXoxcduk5nHraedTXN16cOeH4gYx7+R9c+KszOOWHvwDgqCMH8OH0j/jvr+/Ndv+9N0cf/W169Oje7PVWXXVlPvjgw/IvvUmT61i761rz7HfkEQO47/6HFvBbSfPaeMP1+fsj/wDgxbHjqJsylSlT3+XV19/gvgce4S9XX8ptQ6+gXbt23D2iZT97U995j7XWWA2ADh3as/xyy/LBh9P59NPPGHLDXznxqO/U7PuoaXmVryPi9Yh4MSJGR8QzWdsqETEyIsZnf66ctUdEXB4REyLihYjYquo8A7P9x0fEwIX5u3D29WJi7712ZerUd3nuXy/yP9/870b3u+rqoVx19VD699+fn5/2fY46+hR22+1/2GKLTTnggL0BWLHzCvTccD2mT/+YEfffAsAqK69Ep04d2W+/PgAcceT3qPvSmFtDvv3tA+i99VfZaZcDW+FbSiXHHPYtLvztHzlw4En03KAHm/TcgPbt2vHUM6MZ+8oE+h/9fQBmzJjBKiuvBMD3TjuXyW9NYeasmdRNeYcDB54EwKEH96Xf3rs3eq0rhtzAYYf0Y9lll6n591Ih7ZRSerfq86nAAymlCyPi1Ozzz4A9gZ7Zsi1wFbBtRKwCnAX0pnR317MRMTyl9P6CdGaRB+WIODKldG0j2wYBgwCi/Yq0a7fcIu1bkX39673Zd5/d2bPPziy99FJ07rwCQ6+7nIFHfK/B/W+55U6u+P0FAETAKaecwYiRj8yzX++vlX5ZHX7YwfTo0Y1zf3nZXNtXWmlF2rdvz+zZs+nWtQtvTX67vG2XnXfgtFO/x867HMgXX3zRWl9VYvnlluO800tzHFJK7HHQEXTruhbPPv8S++25Kz844ch5jrn8glJlaHLdFE4//1Ku+8NFc21fY/VVeXvqu6y1xurMmjWbjz/5lJVW7MyLY8Yx8qHHuezKP/PRx58QESzVqRPfPmi/2n9RAYV7S1RfYMdsfSjwMKWg3Be4PpXGPJ6MiJUioku278iU0jSAiBgJ9AEaHmdsRh7l63Ma25BSGpxS6p1S6m1AntvpZ1xIj/V7s+FG2/GdQ0/koYf+MU9A3nDD9crre++1K+MnvAbAiBGPcNxxh9OhQ+nfYD17rt/irODhR57gwANLGfZhh32L4XeNAKBXr8258ooL6XfAkbzzznsL/f2katM/+piZM2cCcNtd97F1ry1Yfrnl2K53L0Y+/Djvvf8BUJql/dbbzVd0AHb6xnbcee/fARjx8GNsu/VXiQiuv+oSRtw2lBG3DeXQg/fn2MMPMSC3HQkYERHPZkkhwJoppbps/W1gzWy9K/Bm1bGTsrbG2hdITTLliHihsU1UvqBawdln/Zhnnn2eu+8eyYknHMEuu+zAzJmz+OD9Dznq6FMA+POQm+jRozujnr6PiODdd6ZxwEFHtej8p/38fG664UrOPfunjH5+DEOuLf3j79cXnMnyyy/HsJv/CMCbb06m3wHzZi9SQ35y1oWM+tcLfPDBdHbZ/1BOPPowZs2aBcAh/fZm4n/e5PTzLiWADdZbl3NPOwWy9e8eeziDTjmd+lRPxw4dOP2HJ7L2Ws3/Wjlgnz047ZcXs+fBR7Fi5xW4+JxTa/gNNT9qcQtTdeU1MzilNPhLu30jpTQ5ItYARkbEK9UbU0opIhZpGh9zZh+26kkjpgB7AF+uqQfwREpp7ebO0aFT10LVM6QF9dlbj+XdBWmhdVxt/ajVuQ9b94BW/33/l//cPl/9jYizgY+BY4EdU0p1WXn64ZTSxhHxx2z95mz/cZRK1ztm+x+Xtc+13/yqVfn6bmD5lNJ/vrS8Tqk+L0lSbiJiuYhYYc46sDvwEjAcmDODeiBwZ7Y+HDg8m4W9HfBhVua+H9g9IlbOZmrvnrUtkJqUr1NKRzex7du1uKYkafGUU1l0TeCOiIBSLLwppXRfRIwCbo2Io4H/AAdn+98L7AVMAD4FjgRIKU2LiF8Co7L9zp0z6WtBeEuUJKnNSSlNBL7aQPt7wC4NtCfgpEbONQQY0tC2+WVQliTlyrdEVfhEL0mSCsJMWZKUq4I9PCRXBmVJUq4W51cttjbL15IkFYSZsiQpV070qjBTliSpIMyUJUm5cqJXhUFZkpQrJ3pVWL6WJKkgzJQlSbmqxdsKF1dmypIkFYSZsiQpV94SVWFQliTlyoleFZavJUkqCDNlSVKuvE+5wkxZkqSCMFOWJOXKiV4VZsqSJBWEmbIkKVc+PKTCoCxJypW3RFVYvpYkqSDMlCVJufKWqAozZUmSCsJMWZKUK2+JqjAoS5Jy5ezrCsvXkiQVhJmyJClXlq8rzJQlSSoIM2VJUq68JarCoCxJylW9E73KLF9LklQQZsqSpFyZJ1eYKUuSVBBmypKkXHlLVIWZsiRJBWGmLEnKlZlyhUFZkpQrn31dYflakqSCMFOWJOXK8nWFmbIkSQVhpixJypXPvq4wKEuScuVErwrL15IkFYSZsiQpV070qjBTliSpIMyUJUm5cky5wqAsScqV5esKy9eSJBWEmbIkKVfep1xhpixJanMiontEPBQRYyNiTER8P2s/OyImR8TobNmr6pjTImJCRIyLiD2q2vtkbRMi4tSF6ZeZsiQpV/X5TPSaBfwopfRcRKwAPBsRI7Ntv0kpXVK9c0RsBvQHNgfWBv4eERtlm68AdgMmAaMiYnhKaeyCdMqgLElqc1JKdUBdtv5RRLwMdG3ikL7AsJTSDOC1iJgAbJNtm5BSmggQEcOyfRcoKFu+liTlKtXgv4gYFBHPVC2DGrt+RPQAtgSeyppOjogXImJIRKyctXUF3qw6bFLW1lj7AjFTliTlqhbl65TSYGBwc/tFxPLAbcApKaXpEXEV8EsgZX9eChzV6h1shEFZktQmRURHSgH5xpTS7QAppSlV268B7s4+Tga6Vx3eLWujifb5ZvlakpSrWpSvmxMRAfwZeDmldFlVe5eq3foBL2Xrw4H+EbFURKwH9ASeBkYBPSNivYjoRGky2PAF/bswU5YktUXbA4cBL0bE6Kzt58CAiOhFqXz9OnAcQEppTETcSmkC1yzgpJTSbICIOBm4H2gPDEkpjVnQTkVRnznaoVPXYnZMmk+fvfVY3l2QFlrH1daPWp17o9V7t/rv+3+/80zN+ltLZsqSpFz5RK8Kx5QlSSoIM2VJUq5yeqJXIZkpS5JUEGbKkqRcOaZcYVCWJOUqpfq8u1AYlq8lSSoIM2VJUq7qLV+XmSlLklQQZsqSpFwV9cmSeTBTliSpIMyUJUm5cky5wqAsScqV5esKy9eSJBWEmbIkKVc++7rCTFmSpIIwU5Yk5cpnX1cYlCVJuXKiV4Xla0mSCsJMWZKUK+9TrjBTliSpIMyUJUm5cky5wqAsScqV9ylXWL6WJKkgzJQlSbmyfF1hpixJUkGYKUuScuUtURVmypIkFYSZsiQpV44pVxiUJUm58paoCsvXkiQVhJmyJClXvrqxwkxZkqSCMFOWJOXKMeUKg7IkKVfOvq6wfC1JUkGYKUuScuVErwozZUmSCsJMWZKUK8eUKwzKkqRcGZQrLF9LklQQZsqSpFyZJ1eYKUuSVBBhLb/tiohBKaXBefdDWlj+LGtJYabctg3KuwNSK/FnWUsEg7IkSQVhUJYkqSAMym2bY3BaUvizrCWCE70kSSoIM2VJkgrCoNxGRUSfiBgXERMi4tS8+yMtiIgYEhFTI+KlvPsitQaDchsUEe2BK4A9gc2AARGxWb69khbIdUCfvDshtRaDctu0DTAhpTQxpfQFMAzom3OfpPmWUnoUmJZ3P6TWYlBum7oCb1Z9npS1SZJyZFCWJKkgDMpt02Sge9XnblmbJClHBuW2aRTQMyLWi4hOQH9geM59kqQ2z6DcBqWUZgEnA/cDLwO3ppTG5Nsraf5FxM3AP4GNI2JSRBydd5+kheETvSRJKggzZUmSCsKgLElSQRiUJUkqCIOyJEkFYVCWJKkgDMoSEBGzI2J0RLwUEX+NiGUX4lzXRcRB2fqfmnrZR0TsGBFfX4BrvB4Rqy1oHyUVk0FZKvkspdQrpfQV4Avg+OqNEdFhQU6aUjompTS2iV12BOY7KEtaMhmUpXk9BmyYZbGPRcRwYGxEtI+IiyNiVES8EBHHAUTJH7L3U/8dWGPOiSLi4Yjona33iYjnIuL5iHggInpQCv4/yLL0HSJi9Yi4LbvGqIjYPjt21YgYERFjIuJPQCzivxNJi8AC/etfWlJlGfGewH1Z01bAV1JKr0XEIODDlNLXImIp4B8RMQLYEtiY0rup1wTGAkO+dN7VgWuAb2bnWiWlNC0irgY+Tildku13E/CblNLjEbEOpaeubQqcBTyeUjo3IvYGfHKVtAQyKEsly0TE6Gz9MeDPlMrKT6eUXsvadwf+a854MbAi0BP4JnBzSmk28FZEPNjA+bcDHp1zrpRSY+8A3hXYLKKcCHeOiOWzaxyQHXtPRLy/YF9TUpEZlKWSz1JKvaobssD4SXUT8N2U0v1f2m+vVuxHO2C7lNLnDfRF0hLOMWWp5e4HToiIjgARsVFELAc8ChySjTl3AXZq4NgngW9GxHrZsatk7R8BK1TtNwL47pwPEdErW30U+HbWtiewcmt9KUnFYVCWWu5PlMaLn4uIl4A/Uqo23QGMz7ZdT+mtRXNJKb0DDAJuj4jngVuyTXcB/eZM9AK+B/TOJpKNpTIL/BxKQX0MpTL2GzX6jpJy5FuiJEkqCDNlSZIKwqAsSVJBGJQlSSoIg7IkSQVhUJYkqSAMypIkFYRBWZKkgjAoS5JUEP8PY0vpiBgskpgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "col = [ i for i in range(0,2)] #類別名稱\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, columns=col, index = col)\n",
    "df_cm.index.name = 'True Label'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing 分布情況 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 22403\n",
      "  Batch size = 64\n",
      "/user_data/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.1710093915462494,\n",
       " 'test_accuracy': 0.9469267508815784,\n",
       " 'test_runtime': 22.1427,\n",
       " 'test_samples_per_second': 1011.753,\n",
       " 'test_steps_per_second': 15.852}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(test_dataset)\n",
    "print('test: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  {'accuracy': 0.9469267508815784}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "print('accuracy : ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.82      3499\n",
      "           1       0.95      0.98      0.97     18904\n",
      "\n",
      "    accuracy                           0.95     22403\n",
      "   macro avg       0.92      0.87      0.89     22403\n",
      "weighted avg       0.95      0.95      0.94     22403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for line in classification_report(labels, predictions).split('\\n'):\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2621   878]\n",
      " [  311 18593]]\n"
     ]
    }
   ],
   "source": [
    "# **產生 confusion matrix heatmap **\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "cf_matrix = confusion_matrix(labels, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7885899871593122\n"
     ]
    }
   ],
   "source": [
    "matthews = matthews_corrcoef(labels, predictions)\n",
    "print(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Predicted', ylabel='True Label'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "col = [ i for i in range(0,2)] #類別名稱\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, columns=col, index = col)\n",
    "df_cm.index.name = 'True Label'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
